{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506386f2",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "A local LLM+RAG chatbot with structured pdf ingestion using Word to convert pdf to docx, and then pandoc to convert from docx to markdown enabling the use of langchain ParentDocumentRetriever with MarkdownTextSplitter.\n",
    "Runs fine on my 64GB RAM laptop under WSL Ubuntu, with 32GB of RAM available to WSL. \n",
    "\n",
    "Most PDF to text parsers do not provide layout information. Often times, even the sentences are split with arbritrary CR/LFs making it very difficult to find paragraph boundaries. This poses various challenges in chunking and adding long running contextual information such as section header to the passages while indexing/vectorizing PDFs for LLM applications such as retrieval augmented generation (RAG).\n",
    "Using Word+Pandoc then ParentDocumentRetriever calling MarkdownTextSplitter chained with RecursiveCharacterTextSplitter solves this problem by parsing PDFs along with layout information.  \n",
    "\n",
    "Replace any path by your own path structure.\n",
    "In addition to Langchain and Chroma this code uses the following Open sources:\n",
    " * Ollama with Wizardlm2 and all-minilm as embedding . [Click here for Ollama website](https://ollama.com/)\n",
    "Wizardlm2 and all-minilm and downloaded locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd99bb",
   "metadata": {},
   "source": [
    "**Split text using Markdown but remove documents that have only formatting characters or not enough words**  \n",
    "Splitting may result on having only table line separators or very short sentences.\n",
    "Removing these for more relevant searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6967e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "from statistics import mean\n",
    "from typing import Any, List, Literal, Optional, Union\n",
    "import re\n",
    "\n",
    "# we can remove markdown from child chunks as we return the parent document\n",
    "# thus vector store does not hold markdown tags \n",
    "def remove_punctuation_and_markdown(input_string):\n",
    "    # Remove markdown formatting characters\n",
    "    input_string = re.sub(r'[_*#|+]', '', input_string)\n",
    "    # Remove punctuation\n",
    "    input_string = re.sub(r'[^\\w\\s]', '', input_string)\n",
    "    return input_string\n",
    "\n",
    "class CleanMarkdownTextSplitter(MarkdownTextSplitter):\n",
    "    \"\"\"Attempts to split the text along Markdown-formatted headings. Only leaving chunks with a meaningful content \"\"\"\n",
    "    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        final_chunks = []\n",
    "        chunks=super()._split_text(text,separators)\n",
    "        for chunk in chunks:\n",
    "            words=chunk.split()\n",
    "            content_len=len(words)\n",
    "            if content_len>0:\n",
    "                meanlength=mean(len(word) for word in words)\n",
    "            else:\n",
    "                meanlength=0\n",
    "            if content_len>10 and meanlength>3:\n",
    "                final_chunks.append(chunk)\n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49298e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Vector store\n",
      "Create document store\n",
      "Start splitting documents\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.storage import LocalFileStore\n",
    "import tempfile,os\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "ollama_ef = OllamaEmbeddings(\n",
    "    model=\"all-minilm\"\n",
    ")\n",
    "\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "\n",
    "# MD splits\n",
    "parent_splitter = MarkdownTextSplitter(chunk_size=5000,chunk_overlap = 200)\n",
    "child_splitter = CleanMarkdownTextSplitter(chunk_size=500,chunk_overlap = 60)\n",
    "\n",
    "md_folder_path = \"/mnt/d/data/md\"\n",
    "print(\"Create Vector store\")\n",
    "vectorstore = Chroma(persist_directory=\"/mnt/d/data/HIE\", embedding_function=ollama_ef)\n",
    "# Instantiate the LocalFileStore with the root path\n",
    "print(\"Create document store\")\n",
    "fs = LocalFileStore(\"/mnt/d/data/documentstore\")\n",
    "store = create_kv_docstore(fs)\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "print(\"Start splitting documents\")\n",
    "loaders = []\n",
    "for i,filename in enumerate(os.listdir(md_folder_path)):\n",
    "    if filename.endswith('.md'):\n",
    "        print(\"load document\",filename)\n",
    "        md_path = os.path.join(md_folder_path, filename)\n",
    "        loader=TextLoader(md_path) \n",
    "        doc=loader.load()\n",
    "        retriever.add_documents(doc)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f0ae2",
   "metadata": {},
   "source": [
    "**Chatbot to interact with documents loaded before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e329cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Query: exit\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "from IPython.core.display import  Markdown \n",
    "from IPython.display import display\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import LocalFileStore\n",
    "import tempfile,os\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "ollama_ef = OllamaEmbeddings(\n",
    "    model=\"all-minilm\"\n",
    ")\n",
    "\n",
    "# MD splits\n",
    "parent_splitter = MarkdownTextSplitter(chunk_size=5000,chunk_overlap = 200)\n",
    "child_splitter = MarkdownTextSplitter(chunk_size=500,chunk_overlap = 60)\n",
    "vectorstore = Chroma(persist_directory=\"/mnt/d/data/HIE\", embedding_function=ollama_ef)\n",
    "# Instantiate the LocalFileStore with the root path\n",
    "fs = LocalFileStore(\"/mnt/d/data/documentstore\")\n",
    "store = create_kv_docstore(fs)\n",
    "llm = Ollama(model=\"wizardlm2\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),num_ctx=4096,verbose=True)\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\n\\nQuery: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    if query.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # Prompt\n",
    "    '''template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "    1. If you don't know the answer, don't try to make up an answer..\n",
    "    2. If you find the answer, write the answer in a concise way\n",
    "    3. Do not give references\n",
    "    4. Use relevant table if available\n",
    "    \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"'''\n",
    "    # Prompt\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "    1. If you don't know the answer, don't try to make up an answer.\n",
    "    2. If you find the answer, write the answer in a detailed way without references.\n",
    "    \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    '''template = \"\"\"Answer the question with details based only on the following context: {context}\n",
    "     \n",
    "     Question: {question}\"\"\"'''\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "        parent_splitter=parent_splitter,\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},return_source_documents=True\n",
    "    )\n",
    "    print(\"====================================\")\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    print(\"\\n\\n Data used\")\n",
    "    for i,doc in enumerate(result.get(\"source_documents\", [])):\n",
    "        if \"source\" in doc.metadata:\n",
    "            print(i+1,os.path.basename(doc.metadata[\"source\"]))\n",
    "        else:\n",
    "            print(\"no source\")\n",
    "\n",
    "        display(Markdown(doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c704b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
