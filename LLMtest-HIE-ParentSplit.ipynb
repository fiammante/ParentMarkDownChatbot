{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506386f2",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "A local LLM+RAG chatbot with structured pdf ingestion using Word to convert pdf to docx, and then pandoc to convert from docx to markdown enabling the use of langchain ParentDocumentRetriever with MarkdownTextSplitter.\n",
    "Runs fine on my 64GB RAM laptop under WSL Ubuntu, with 32GB of RAM available to WSL. \n",
    "\n",
    "Most PDF to text parsers do not provide layout information. Often times, even the sentences are split with arbritrary CR/LFs making it very difficult to find paragraph boundaries. This poses various challenges in chunking and adding long running contextual information such as section header to the passages while indexing/vectorizing PDFs for LLM applications such as retrieval augmented generation (RAG).\n",
    "Using Word+Pandoc then ParentDocumentRetriever calling MarkdownTextSplitter chained with RecursiveCharacterTextSplitter solves this problem by parsing PDFs along with layout information.  \n",
    "\n",
    "Replace any path by your own path structure.\n",
    "In addition to Langchain and Chroma this code uses the following Open sources:\n",
    " * Ollama with Wizardlm2. [Click here for Ollama website](https://ollama.com/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49298e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load document 1-s2.0-S0169260721006672-main.md\n",
      "load document 1-s2.0-S0987705310000080-main.md\n",
      "load document 1-s2.0-S098770532030109X-am.md\n",
      "load document 1-s2.0-S0987705320301477-am.md\n",
      "load document 1-s2.0-S1388245715006136-main.md\n",
      "load document 1-s2.0-S1388245715006215-main.md\n",
      "load document 1-s2.0-S2405844021015140-main.md\n",
      "load document 10.2478_prilozi-2022-0013.md\n",
      "load document 12519_2023_Article_698.md\n",
      "load document 2106.00061.md\n",
      "load document 217656905.md\n",
      "load document 70581176.md\n",
      "load document ACI-Hypoxic-ischaemic-encephalopathy-in-newborns-recognition-monitoring-and-early-management.md\n",
      "load document Acta Paediatrica - July 1955 - ENHORNING - An Experimental Study of the Human Fetus with Special Reference to Asphyxia.md\n",
      "load document aeeg.md\n",
      "load document Analyse quantitative et automatisée des EEG néonataux post-anoxiques.md\n",
      "load document app7-parent-info.md\n",
      "load document battisti_pediaelectrophysiology.md\n",
      "load document children-09-01194-v2.md\n",
      "load document chp_NE.md\n",
      "load document e023301.md\n",
      "load document encoches1.md\n",
      "load document fped-11-1138062.md\n",
      "load document GarveyAA_PhD2022.md\n",
      "load document GRAIC Solène. Thèse d'exercice Médecine Pédiatrie (UPJV).md\n",
      "load document Guideline-16.md\n",
      "load document hypothermia.md\n",
      "load document ijms-21-01487.md\n",
      "load document ilse_tse_thesis_final.md\n",
      "load document jne_18_4_046007.md\n",
      "load document journal.pone.0291170.md\n",
      "load document Kane_1-s2.0-S2467981X17300215-main.md\n",
      "load document Karoline Aker.md\n",
      "load document Murray_Mild_HIE_paper.md\n",
      "load document Neonatal EEG Manuscript_Pre review.md\n",
      "load document neonatal-therapeutic-hypothermia-in-ireland-annual-report-2019.md\n",
      "load document nihms-819104.md\n",
      "load document nihms203543.md\n",
      "load document nihms310162.md\n",
      "load document ProjetThese_Marc_2021.md\n",
      "load document Raurale_2021_J._Neural_Eng._18_046007.md\n",
      "load document S0987705320301118.md\n",
      "load document S2341287922001314.md\n",
      "load document s4118993_phd_final.md\n",
      "load document s41372-021-01132-4.md\n",
      "load document s41390-019-0693-0.md\n",
      "load document s41597-023-02002-8.md\n",
      "load document sara,+1407-4996-1-CE.md\n",
      "load document Thesis_Olson.md\n",
      "load document Tsuchida-ACNSStandardizedEEGTerminologyforthedescriptionofcEEGinneonates2013.md\n",
      "load document ucalgary_2023_randhawa_alam.md\n",
      "load document z000-Normal-EEG-in-newborn-Handout.md\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.storage import LocalFileStore\n",
    "import tempfile,os\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "\n",
    "# MD splits\n",
    "parent_splitter = MarkdownTextSplitter()\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# Char-level splits\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 60\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "md_folder_path = \"/mnt/d/data/md\"\n",
    "vectorstore = Chroma(persist_directory=\"/mnt/d/data/MDHIE\", embedding_function=GPT4AllEmbeddings())\n",
    "# Instantiate the LocalFileStore with the root path\n",
    "fs = LocalFileStore(\"/mnt/d/data/documentstore\")\n",
    "store = create_kv_docstore(fs)\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "loaders = []\n",
    "for i,filename in enumerate(os.listdir(md_folder_path)):\n",
    "    if filename.endswith('.md'):\n",
    "        print(\"load document\",filename)\n",
    "        md_path = os.path.join(md_folder_path, filename)\n",
    "        loader=TextLoader(md_path) \n",
    "        doc=loader.load()\n",
    "        retriever.add_documents(doc)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e329cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Query: exit\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "import os\n",
    "from IPython.core.display import  Markdown \n",
    "from IPython.display import display\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import LocalFileStore\n",
    "import tempfile,os\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "# MD splits\n",
    "parent_splitter = MarkdownTextSplitter()\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "vectorstore = Chroma(persist_directory=\"/mnt/d/data/MDHIE\", embedding_function=GPT4AllEmbeddings())\n",
    "# Instantiate the LocalFileStore with the root path\n",
    "fs = LocalFileStore(\"/mnt/d/data/documentstore\")\n",
    "store = create_kv_docstore(fs)\n",
    "llm = Ollama(model=\"wizardlm2\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),num_ctx=4096,verbose=True)\n",
    "#llm = Ollama(model=\"orca-mini:13b\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),num_ctx=4096,verbose=True)\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\n\\nQuery: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    if query.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # Prompt\n",
    "    '''template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "    1. If you don't know the answer, don't try to make up an answer..\n",
    "    2. If you find the answer, write the answer in a concise way\n",
    "    3. Do not give references\n",
    "    4. Use relevant table if available\n",
    "    \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"'''\n",
    "    # Prompt\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "    1. If you don't know the answer, don't try to make up an answer.\n",
    "    2. If you find the answer, write the answer in a detailed way without references.\n",
    "    \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    '''template = \"\"\"Answer the question with details based only on the following context: {context}\n",
    "     \n",
    "     Question: {question}\"\"\"'''\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "        parent_splitter=parent_splitter)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},return_source_documents=True\n",
    "    )\n",
    "    print(\"====================================\")\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    print(\"\\n\\n Data used\")\n",
    "    for i,doc in enumerate(result.get(\"source_documents\", [])):\n",
    "        if \"source\" in doc.metadata:\n",
    "            print(i+1,os.path.basename(doc.metadata[\"source\"]))\n",
    "        else:\n",
    "            print(\"no source\")\n",
    "\n",
    "        display(Markdown(doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d7a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
